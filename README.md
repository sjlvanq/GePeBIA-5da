# GePeBIA: Collaborative Multi-Agent System for Small Popular and Community Libraries (Capstone Project)

## üåü Project Overview: A Level 3 Collaborative Architecture

This project implements the design principles of a **Level 3: Collaborative Multi-Agent System** [4, p. 17], which moves beyond a monolithic design toward a **"team of specialists"** working in concert [4, p. 17; 4, p. 16]. The system's robustness and scalability are achieved through a clear **division of labor** [4, p. 17].

The architecture follows the **Core Agent Anatomy** [4, p. 19]: the reasoning Model (Brain), external Tools (Hands), and the Orchestration Layer [4, p. 19; 4, p. 20]:

| Agent | Role in the System | Core Architectural Concept | Function & Capability |
| :--- | :--- | :--- | :--- |
| **Iris** | **Orchestrator / User Experience (UX)** | **Orchestration Layer** [4, p. 22] | Manages the primary conversational flow and maintains the immediate **Session** history [3, p. 6]. |
| **Alec** | **Logistics and Inventory Specialist** | **Tools** (Function Calling) [4, p. 20; 2, p. 10] | Executes deterministic business logic, such as checking material availability and calculating loan terms. |
| **Gina** | **User Profile Manager** | **Memory** (Long-Term Persistence) [3, p. 6] | Handles multi-step user registration and manages persistent user profiles and preferences [3, p. 38]. |

## ü§ù Interoperability and Protocols

The system's collaboration relies on standardized mechanisms for communication and tool integration, adhering to AgentOps discipline [4, p. 27]:

### 1. Agent2Agent (A2A) Communication

The integration of `Alec` and `Gina` as specialized services consumed by `Iris` is achieved using the **Agent2Agent (A2A) Protocol** [5, p. 26; 5, p. 27].

*   **Decoupling:** A2A allows these specialized agents to run as independent services (simulating cross-team or cross-organization integration) [5, p. 27; 5, p. 26].
*   **Agent-as-a-Tool:** `Iris` integrates the remote agents by treating them as callable tools via the `RemoteA2aAgent` class [2, p. 13].

### 2. Structured Tool Design

The reliability of the system is improved by rigorous constraints on tool output and messaging:

*   **Design for Concise Output:** Tools must return small, structured JSON objects rather than large text blocks [2, p. 19]. This best practice prevents **Context Window Bloat**, which degrades the model's reasoning quality, increases latency, and raises token costs [2, p. 19].
*   **Describe Actions, Not Implementations:** The orchestrator's instructions must prompt the model on *what* needs to be accomplished (e.g., "Check availability") instead of specifying *how* to call the underlying tool implementation [2, p. 17; 2, p. 18].

## üõ°Ô∏è Quality, Observability, and Safety

The system is engineered for trustworthiness, operating under the principle that **Agent quality is an architectural pillar, not a final testing phase** [1, p. 46; 4, p. 6].

### 1. The Trajectory is the Truth

The specialist agents (`Alec`, `Gina`) are fundamentally constrained to enforce predictable behavior: they **MUST ALWAYS** call one tool and **MUST ONLY** return the exact, unmodified JSON output generated by that tool.

*   This rule implements the concept that **The Trajectory is the Truth** [1, p. 46; 1, p. 6]. Since the LLM's logic is non-deterministic [1, p. 11], the evaluation must assess the agent's entire decision-making process [1, p. 19; 1, p. 16].
*   This design makes the system **evaluatable-by-design** [1, p. 46], facilitating **Trajectory Evaluation** (the "Glass Box" approach) to audit reasoning, tool use, and robustness [1, p. 19; 4, p. 29].

### 2. Observability Foundation

Observability provides the essential technical foundation for judging the agent's complex process [1, p. 6; 1, p. 31].

*   **Pillars of Observability:** The practice is built on **Logs** (atomic event records or the agent's diary) [1, p. 32; 1, p. 33], **Traces** (the narrative thread linking events to show cause and effect) [1, p. 32; 1, p. 36], and **Metrics** (the aggregated health report) [1, p. 32; 1, p. 38].
*   **Tracing** is indispensable for answering "Why?" when a system fails, by visually inspecting the entire execution path, including model and tool calls [1, p. 36; 4, p. 30].

### 3. Context, Memory, and HITL Safety

*   **Memory Isolation:** The system utilizes **Memory** for long-term knowledge (user profiles) [3, p. 6], which is defined as being **highly isolated per-user** [3, p. 31] to prevent data leakage and ensure privacy [3, p. 69].
*   **Human-in-the-Loop (HITL):** The **Golden Rule of Registration** in `Iris` enforces a mandatory confirmation step (`wait_for_user_confirmation`) before initiating a sensitive action like profile creation [203; 204]. This process integrates **Human-in-the-Loop (HITL) judgment** [1, p. 26; 1, p. 46] to anchor the system's safety and alignment with governance policies [1, p. 41].

-----

## Agent ports and run instructions

This project contains multiple agents. Below are the default ports used during local testing and example commands to start each agent. Adjust ports and environment settings as needed for your setup.

Default ports used in local tests
- Alec (a2a agent): 8001
- Gina (a2a agent): 8003
- Iris (user-facing web agent / ADK web): uses the ADK web default (commonly 8000); in my tests I started it without explicitly passing a port.

Notes:
- Commands below assume you run them from the agent's folder (specified for each command).
- The examples use a relative .env file located at the repository root (../.env from each agent folder). Update the path if your .env is elsewhere.
- On Windows you may use the uvicorn.exe executable if installed; on other platforms use python -m uvicorn or uvicorn.

Run commands (examples)

1) Start Gina (from ./gina)
```bash
# from the project root:
cd ./gina

# Windows example
uvicorn.exe --port 8003 --env-file ../.env --log-level=debug gina_a2a:a2a_app

# Cross-platform alternative
python -m uvicorn gina_a2a:a2a_app --port 8003 --env-file ../.env --log-level debug
```

2) Start Alec (from ./alec)
```bash
# from the project root:
cd ./alec

# Windows example
uvicorn.exe --port 8001 --env-file ../.env --log-level=debug alec_a2a:a2a_app

# Cross-platform alternative
python -m uvicorn alec_a2a:a2a_app --port 8001 --env-file ../.env --log-level debug
```

3) Start Iris (user-facing web agent, from repository root)
```bash
# from the project root:
# start the ADK web UI for Iris (debug logging)
adk web --log_level=debug .

# If you need to force a port for ADK web, consult ADK docs or
# set the corresponding environment variable (for example ADK_PORT)
# or pass the port flag if supported by your ADK version.
```

Quick test and verification
- Open the web UI for Iris in your browser (usually http://localhost:8000 unless overridden).
- Verify Alec and Gina are up:
  - Visit http://localhost:8001/.well-known/agent-card.json and http://localhost:8003/.well-known/agent-card.json respectively.
  - Check the terminal logs for startup messages and any registration/connection traces.
- Example curl (replace path if your agent exposes different endpoints):
```bash
curl -v http://localhost:8001/.well-known/agent-card.json
curl -v http://localhost:8003/.well-known/agent-card.json
```

- Environment file (.env): commands above use `--env-file ../.env` when starting from each agent folder. Ensure the .env file exists and contains required variables.
- If uvicorn is not found, use `python -m uvicorn` or install uvicorn in the active environment: `pip install uvicorn`.
- ADK web flags/overrides: ADK CLI options may vary by version. If you need to bind Iris to a specific port, check `adk --help` or ADK documentation for the exact flag or environment variable to set.

***

## üìù Bibliography

1. Meltem Subasioglu, Turan Bulmus, and Wafae Bakkali, *Agent Quality* (Google, November 2025).
2. Mike Styer, Kanchana Patlolla, Madhuranjan Mohan, and Sal Diaz, *Agent Tools & Interoperability with Model Context Protocol (MCP)* (Google, November 2025).
3. Kimberly Milam and Antonio Gulli, *Context Engineering: Sessions, Memory* (Google, November 2025).
4. Alan Blount, Antonio Gulli, Shubham Saboo, Michael Zimmermann, and Vladimir Vuskovic, *Introduction to Agents* (Google, November 2025).
5. Sokratis Kartakis, Gabriela Hernandez Larios, Ran Li, Elia Secchi, and Huang Xia, *Prototype to Production* (Google, November 2025).
